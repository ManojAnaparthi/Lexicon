# ğŸ” Lexicon: Word Embedding Based Search Engine

**Lexicon** is a semantic search engine built using modern word embeddings and hybrid retrieval methods. Built on a 100,000-passage subset of the **MS MARCO** dataset, this project evaluates and compares multiple retrieval pipelines:

- **Word2Vec only**
- **Word2Vec + BERT re-ranking**
- **Word2Vec + BM25 re-ranking**

## ğŸ“Š Results at a Glance

| Retrieval Setup       | MRR@10       | Recall@10   | NDCG@10     |
|-----------------------|--------------|-------------|-------------|
| Word2Vec              | 0.1568       | 0.450       | 0.2244      |
| Word2Vec + **BERT**   | **0.1983**   | **0.525**   | **0.2728**  |
| Word2Vec + BM25       | 0.1701       | 0.478       | 0.2404      |

## ğŸ¥ Live Demo

ğŸ‘‰ [Streamlit Demo Video]([https://iitgnacin-my.sharepoint.com/:v:/g/personal/23110071_iitgn_ac_in/Eetee1CE1HdKpPXwFzSvb84BzeRomRpsBKF4EnEJVmJzLA?e=XAthoS](https://iitgnacin-my.sharepoint.com/:v:/g/personal/23110025_iitgn_ac_in/EResIprMMyJJhK52XxIrGIMBf4Hs5K4U9RNmkhnDAvfvZw?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=ANJs5f))

## âš™ï¸ Features

- ğŸ”¹ Fast semantic retrieval with **Word2Vec** and **FAISS**
- ğŸ”¹ Enhanced re-ranking using **Sentence-BERT (all-MiniLM-L6-v2)**
- ğŸ”¹ Traditional lexical matching with **BM25**
- ğŸ”¹ Clustering and visualization with **t-SNE** and **K-Means**

## ğŸ›  Methodology

### 1. Word2Vec Baseline
- 300-dim embeddings trained with Gensim
- Passage representation = mean of word vectors
- Approximate nearest neighbors via FAISS

### 2. Word2Vec + BERT
- Initial candidates from FAISS + Word2Vec
- Re-ranked using cosine similarity of Sentence-BERT embeddings

### 3. Word2Vec + BM25
- Combined ranking using normalized FAISS scores and BM25 scores

## ğŸ“ Repository Structure

Lexicon/<br>
â”œâ”€â”€ data/            # Processed MS MARCO subset and utility data files (**This has been done locally and the dataset size was too large to push to Github**)<br>
â”œâ”€â”€ model/           # Saved Word2Vec and BERT models<br>
â”œâ”€â”€ notebook/        # Jupyter notebooks for development and analysis<br>
â”œâ”€â”€ web_app/        # Streamlit app code for demo<br>
â”œâ”€â”€ Project_Report_Lexicon.pdf  # Final report<br>
â”œâ”€â”€ requirements.txt # Python dependencies<br>
â””â”€â”€ README.md       # You're here


## ğŸ“š Tools & Libraries

- [MS MARCO](https://huggingface.co/datasets/msmarco)
- [Gensim](https://radimrehurek.com/gensim/models/word2vec.html) - Word2Vec
- [FAISS](https://faiss.ai/) - Fast similarity search
- [NLTK](https://www.nltk.org/) - Text preprocessing
- [SentenceTransformers](https://www.sbert.net/) - BERT embeddings
- [Scikit-learn](https://scikit-learn.org/) - Clustering
- [Matplotlib](https://matplotlib.org/) - Visualization

## ğŸ“ˆ Evaluation Metrics

- **MRR@10** - Mean Reciprocal Rank
- **Recall@10**
- **NDCG@10**
- **Semantic Similarity (Mean & Max)**
- **Hit Rate@5**

## ğŸ‘¥ Contributors

- **A.V.S Manoj** (23110025) â€“ [manoj.anaparthi@iitgn.ac.in](mailto:manoj.anaparthi@iitgn.ac.in)  
- **B. Saharsh** (23110071) â€“ [burra.saharsh@iitgn.ac.in](mailto:burra.saharsh@iitgn.ac.in)  
- **O. Akash** (23110225) â€“ [23110225@iitgn.ac.in](mailto:23110225@iitgn.ac.in)

---
